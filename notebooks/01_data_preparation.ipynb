{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"notebooks\"), '..')))\n",
    "\n",
    "from utils.func_preprocessing import (\n",
    "    calculate_age,\n",
    "    calculate_pf,\n",
    "    calculate_podt,\n",
    "    classify_cod_patology\n",
    ")\n",
    "\n",
    "from config.config import (\n",
    "    DATASET_NAME,\n",
    "    YEARS_TRAINING\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define columns to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_emogas = [\n",
    "    'NCAMPIONE', 'ID', \"NOME\", \"COGNOME\", \"DATA_NASCITA\", 'NACCESSO', 'DATA',\n",
    "    'TIPO', 'STATO', 'REPARTO_PZ', 'SESSO', 'ANNI', 'AADPO2', 'PF', 'PO2_T',\n",
    "    'P50_ACT', 'TO2', 'SO2', 'PO2.1', 'HCT', 'AG_K', 'THB', 'THB2', 'GLU', 'LAC',\n",
    "    'HCO3', 'PCO2_T', 'PCO2', 'MOSM', 'KP', 'NA', 'CL', 'CBASE', 'METHB', 'PHT',\n",
    "    'PH', 'O2HB', 'COHB', 'RHB', 'B', 'TC', 'FIO2', 'P50_ST', 'RQ', 'P50', 'PEEP',\n",
    "    'PS', 'CA'\n",
    "    ]\n",
    "\n",
    "columns_ps = [\n",
    "    'ID_ANAGRAFICA', \"NOME\", \"COGNOME\", 'DTN', 'SESSO', 'DATA_INGRESSO',\n",
    "    'DATA_USCITA', 'COD_PATOLOGIA', 'COD_DIAGNOSI_PRINCIPALE'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store DataFrames\n",
    "df_emogas_list = []\n",
    "\n",
    "# Loop through each year, read the corresponding Excel file, and append to the list\n",
    "for year in YEARS_TRAINING:\n",
    "    file_path = f\"../raw_dataset/Import_Emogas_{year}.xlsx\"\n",
    "    df_year = pd.read_excel(file_path)[columns_emogas]\n",
    "    # Rename 'PO2.1' to 'PO2' if it exists\n",
    "    if 'PO2.1' in df_year.columns:\n",
    "        df_year = df_year.rename(columns={'PO2.1': 'PO2'})\n",
    "    df_emogas_list.append(df_year)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "df_emogas = pd.concat(df_emogas_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store DataFrames\n",
    "df_ps_list = []\n",
    "\n",
    "# Loop through each year, read the corresponding Excel file, and append to the list\n",
    "for year in YEARS_TRAINING:\n",
    "    file_path = f\"../raw_dataset/RIEP_{year}.xlsx\"\n",
    "    df_year = pd.read_excel(file_path)[columns_ps]\n",
    "    df_ps_list.append(df_year)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "df_ps = pd.concat(df_ps_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original DataFrame\n",
    "df_temp_ps = df_ps.copy()\n",
    "\n",
    "# Extract components from 'COD_DIAGNOSI_PRINCIPALE'\n",
    "df_temp_ps['lettera_diagnosi_princip'] = df_temp_ps['COD_DIAGNOSI_PRINCIPALE'].str.extract(r'([A-Za-z]+)', expand=False)\n",
    "\n",
    "# Extract the integer part of the diagnosis code\n",
    "df_temp_ps['intero_diagnosi_princip'] = df_temp_ps['COD_DIAGNOSI_PRINCIPALE'].str.extract(r'(\\d+)\\.', expand=False)\n",
    "df_temp_ps['intero_diagnosi_princip'] = df_temp_ps['intero_diagnosi_princip'].fillna(\n",
    "    df_temp_ps['COD_DIAGNOSI_PRINCIPALE'].str.extract(r'(\\d+)', expand=False)\n",
    ")\n",
    "\n",
    "# Extract the decimal part of the diagnosis code\n",
    "df_temp_ps['decimali_diagnosi_princip'] = df_temp_ps['COD_DIAGNOSI_PRINCIPALE'].str.extract(r'\\.(\\d+)', expand=False)\n",
    "df_temp_ps['decimali_diagnosi_princip'] = df_temp_ps['decimali_diagnosi_princip'].fillna(\n",
    "    df_temp_ps['COD_DIAGNOSI_PRINCIPALE'].apply(lambda x: '0' if pd.notna(x) and '.' not in str(x) else None)\n",
    ")\n",
    "\n",
    "# Calculate age using a helper function\n",
    "df_temp_ps['ETA'] = df_temp_ps.apply(\n",
    "    lambda row: calculate_age(row['DTN'], row['DATA_INGRESSO']), axis=1\n",
    ")\n",
    "\n",
    "# Select relevant columns\n",
    "columns_to_keep = [\n",
    "    'ID_ANAGRAFICA', \"NOME\", \"COGNOME\", 'DTN', 'SESSO', 'ETA', \n",
    "    'DATA_INGRESSO', 'DATA_USCITA', 'COD_PATOLOGIA', 'COD_DIAGNOSI_PRINCIPALE', \n",
    "    'lettera_diagnosi_princip', 'intero_diagnosi_princip', 'decimali_diagnosi_princip'\n",
    "]\n",
    "df_temp_ps = df_temp_ps[columns_to_keep]\n",
    "\n",
    "# Filter out rows with null 'COD_DIAGNOSI_PRINCIPALE'\n",
    "df_temp_ps = df_temp_ps.loc[df_temp_ps['COD_DIAGNOSI_PRINCIPALE'].notnull()]\n",
    "\n",
    "# Fill missing 'lettera_diagnosi_princip' values with \"N\"\n",
    "df_temp_ps['lettera_diagnosi_princip'] = df_temp_ps['lettera_diagnosi_princip'].fillna(\"N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the Emogas dataset\n",
    "df_temp_emogas = df_emogas[\n",
    "    [\n",
    "        'NCAMPIONE', 'ID', 'NOME', 'COGNOME', 'DATA_NASCITA', 'NACCESSO', 'DATA',\n",
    "        'TIPO', 'STATO', 'REPARTO_PZ', 'SESSO', 'ANNI',\n",
    "        'AADPO2', 'PF', 'PO2_T', 'P50_ACT', 'TO2', 'SO2', 'PO2',  # PO2 added, SO21 replaced with SO2\n",
    "        'HCT', 'AG_K', 'THB', 'THB2', 'GLU', 'LAC', 'HCO3', 'PCO2_T', 'PCO2', \n",
    "        'MOSM', 'KP', 'NA', 'CL', 'CBASE', 'METHB', 'PHT', 'PH', 'O2HB', 'COHB', \n",
    "        'RHB', 'B', 'TC', 'FIO2', 'P50_ST', 'RQ', 'P50', 'PEEP', 'PS', 'CA'\n",
    "    ]\n",
    "].rename(\n",
    "    columns={\"ID\": \"ID_ANAGRAFICA\", \"DATA_NASCITA\": \"DTN\"}\n",
    ")\n",
    "\n",
    "# Filter rows with valid STATO values\n",
    "df_temp_emogas = df_temp_emogas[df_temp_emogas['STATO'].isin(['?', 'OK'])]\n",
    "\n",
    "# Drop rows with missing critical fields\n",
    "required_fields = ['NOME', 'COGNOME', 'ID_ANAGRAFICA']\n",
    "df_temp_emogas = df_temp_emogas.dropna(subset=required_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data types for ID_ANAGRAFICA to string\n",
    "df_temp_ps['ID_ANAGRAFICA'] = df_temp_ps['ID_ANAGRAFICA'].astype(str)\n",
    "df_temp_emogas['ID_ANAGRAFICA'] = df_temp_emogas['ID_ANAGRAFICA'].astype(str)\n",
    "\n",
    "# Convert date columns to datetime format in df_temp_ps\n",
    "date_columns_ps = ['DATA_INGRESSO', 'DATA_USCITA', 'DTN']\n",
    "df_temp_ps[date_columns_ps] = df_temp_ps[date_columns_ps].apply(pd.to_datetime)\n",
    "\n",
    "# Convert date column to datetime format in df_temp_emogas\n",
    "df_temp_emogas['DATA'] = pd.to_datetime(df_temp_emogas['DATA'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_temp_ps and df_temp_emogas on 'ID_ANAGRAFICA'\n",
    "merged_df_id = pd.merge(\n",
    "    df_temp_ps,\n",
    "    df_temp_emogas.drop(columns=[\"NOME\", \"COGNOME\", \"DTN\", \"SESSO\"]),\n",
    "    on='ID_ANAGRAFICA',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Filter rows where 'DATA' falls within the range of 'DATA_INGRESSO' and 'DATA_USCITA'\n",
    "result_id = merged_df_id[\n",
    "    (merged_df_id['DATA'] >= merged_df_id['DATA_INGRESSO']) &\n",
    "    (merged_df_id['DATA'] <= merged_df_id['DATA_USCITA'])\n",
    "]\n",
    "\n",
    "# Sort values by 'ID_ANAGRAFICA', 'DATA_INGRESSO', and 'DATA'\n",
    "result_id = result_id.sort_values(by=[\"ID_ANAGRAFICA\", \"DATA_INGRESSO\", \"DATA\"])\n",
    "\n",
    "# Remove duplicates, keeping the first occurrence for each 'ID_ANAGRAFICA' and 'DATA_INGRESSO'\n",
    "result_id = result_id.drop_duplicates(subset=[\"ID_ANAGRAFICA\", \"DATA_INGRESSO\"], keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_temp_ps and df_temp_emogas on 'NOME' and 'COGNOME'\n",
    "merged_df_name = pd.merge(\n",
    "    df_temp_ps,\n",
    "    df_temp_emogas.drop(columns=[\"ID_ANAGRAFICA\", \"DTN\", \"SESSO\"]),\n",
    "    on=[\"NOME\", \"COGNOME\"],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Filter rows where 'DATA' falls within the range of 'DATA_INGRESSO' and 'DATA_USCITA'\n",
    "result_name = merged_df_name[\n",
    "    (merged_df_name['DATA'] >= merged_df_name['DATA_INGRESSO']) &\n",
    "    (merged_df_name['DATA'] <= merged_df_name['DATA_USCITA'])\n",
    "]\n",
    "\n",
    "# Sort values by 'NOME', 'COGNOME', 'DATA_INGRESSO', and 'DATA'\n",
    "result_name = result_name.sort_values(by=[\"NOME\", \"COGNOME\", \"DATA_INGRESSO\", \"DATA\"])\n",
    "\n",
    "# Remove duplicates, keeping the first occurrence for each 'NOME', 'COGNOME', and 'DATA_INGRESSO'\n",
    "result_name = result_name.drop_duplicates(subset=[\"NOME\", \"COGNOME\", \"DATA_INGRESSO\"], keep='first')\n",
    "\n",
    "# Concatenate result_id and result_name DataFrames\n",
    "concat = pd.concat([result_id, result_name])\n",
    "\n",
    "# Drop duplicate rows across both merges\n",
    "df_final = concat.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and reorder columns in the final DataFrame\n",
    "df_final = df_final[\n",
    "    [\n",
    "        'ID_ANAGRAFICA', 'DTN', 'SESSO', 'ETA', \n",
    "        'DATA_INGRESSO', 'DATA', 'DATA_USCITA', \n",
    "        'NCAMPIONE', 'NACCESSO', 'TIPO', 'STATO', 'REPARTO_PZ',\n",
    "        'COD_PATOLOGIA', 'AADPO2', 'PF', 'PO2_T', 'P50_ACT',\n",
    "        'TO2', 'SO2', 'HCT', 'AG_K', 'THB', 'THB2', 'GLU', \n",
    "        'LAC', 'PO2',  # Removed PO21 and replaced SO21 with SO2\n",
    "        'HCO3', 'PCO2_T', 'PCO2', 'MOSM', 'KP', 'NA', 'CL', \n",
    "        'CBASE', 'METHB', 'PHT', 'PH', 'O2HB', 'COHB', \n",
    "        'RHB', 'B', 'TC', 'FIO2', 'P50_ST', 'RQ', 'P50', \n",
    "        'PEEP', 'PS', 'CA',  # Added CA\n",
    "        'COD_DIAGNOSI_PRINCIPALE', 'lettera_diagnosi_princip', \n",
    "        'intero_diagnosi_princip', 'decimali_diagnosi_princip'\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 'PO2_T_calc' based on 'PO2' and 'TC', and update 'PO2_T' if missing\n",
    "df_final[\"PO2_T_calc\"] = df_final.apply(\n",
    "    lambda row: calculate_podt(row['PO2'], row['TC']) \n",
    "    if pd.notnull(row['PO2']) and pd.notnull(row['TC']) and row['TC'] != 0 and row['PO2'] != '.....' \n",
    "    else None, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Fill missing values in 'PO2_T' with 'PO2_T_calc' and drop the temporary column\n",
    "df_final['PO2_T'] = df_final['PO2_T'].fillna(df_final['PO2_T_calc'])\n",
    "df_final = df_final.drop(columns=[\"PO2_T_calc\"])\n",
    "\n",
    "# Calculate 'PF' based on 'PO2_T' and 'FIO2'\n",
    "df_final[\"PF\"] = df_final.apply(\n",
    "    lambda row: calculate_pf(row['PO2_T'], row['FIO2']) \n",
    "    if pd.notnull(row['PO2_T']) and pd.notnull(row['FIO2']) and row['FIO2'] != 0 \n",
    "    else None, \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original DataFrame\n",
    "cleaned_df = df_final.copy()\n",
    "\n",
    "# Initialize a DataFrame to collect \"dirty\" rows and their reasons\n",
    "dirty_df = pd.DataFrame()\n",
    "\n",
    "# Helper function to mark and collect dirty rows\n",
    "def collect_dirty_rows(df, condition, reason):\n",
    "    tmp_dirty = df.loc[condition].copy()\n",
    "    tmp_dirty[\"Motivo\"] = reason\n",
    "    return tmp_dirty\n",
    "\n",
    "# 1. Handle `ETA` (Age)\n",
    "dirty_df = pd.concat([dirty_df, collect_dirty_rows(cleaned_df, cleaned_df.ETA == 0, \"ETA uguale a 0\")])\n",
    "cleaned_df = cleaned_df.loc[cleaned_df.ETA > 0]\n",
    "\n",
    "# 2. Handle `GLU`\n",
    "dirty_df = pd.concat([dirty_df, collect_dirty_rows(cleaned_df, cleaned_df.GLU == '.....', \"GLU == .....\")])\n",
    "cleaned_df = cleaned_df.loc[cleaned_df.GLU != '.....']\n",
    "\n",
    "cleaned_df[\"GLU\"] = cleaned_df[\"GLU\"].astype(float)\n",
    "dirty_df = pd.concat([dirty_df, collect_dirty_rows(cleaned_df, cleaned_df.GLU <= 6, \"GLU minore di 6\")])\n",
    "dirty_df = pd.concat([dirty_df, collect_dirty_rows(cleaned_df, cleaned_df.GLU.isnull(), \"GLU nullo\")])\n",
    "cleaned_df = cleaned_df.loc[cleaned_df.GLU > 6]\n",
    "\n",
    "# 3. Handle `PCO2_T`\n",
    "dirty_df = pd.concat([dirty_df, collect_dirty_rows(cleaned_df, cleaned_df.PCO2_T < 10, \"PCO2_T minore di 10\")])\n",
    "dirty_df = pd.concat([dirty_df, collect_dirty_rows(cleaned_df, cleaned_df.PCO2_T.isnull(), \"PCO2_T nullo\")])\n",
    "cleaned_df = cleaned_df.loc[cleaned_df.PCO2_T > 10]\n",
    "\n",
    "# 4. Handle `TC`\n",
    "dirty_df = pd.concat([dirty_df, collect_dirty_rows(cleaned_df, cleaned_df.TC < 33, \"TC minore di 33\")])\n",
    "cleaned_df = cleaned_df.loc[cleaned_df.TC >= 33]\n",
    "\n",
    "# 5. Handle `FIO2`\n",
    "dirty_df = pd.concat([dirty_df, collect_dirty_rows(cleaned_df, cleaned_df.FIO2 < 21, \"FIO2 minore di 21\")])\n",
    "cleaned_df = cleaned_df.loc[cleaned_df.FIO2 >= 21]\n",
    "\n",
    "# 6. Handle `TIPO`\n",
    "dirty_df = pd.concat([dirty_df, collect_dirty_rows(cleaned_df, ~cleaned_df.TIPO.isin([\"Arterioso\", \"Venoso\"]), \"TIPO inaspettato\")])\n",
    "cleaned_df = cleaned_df.loc[cleaned_df.TIPO.isin([\"Arterioso\", \"Venoso\"])]\n",
    "\n",
    "# 7. Handle `THB`\n",
    "dirty_df = pd.concat([dirty_df, collect_dirty_rows(cleaned_df, cleaned_df.THB == '.....', \"THB == .....\")])\n",
    "cleaned_df = cleaned_df.loc[cleaned_df.THB != '.....']\n",
    "\n",
    "dirty_df = pd.concat([dirty_df, collect_dirty_rows(cleaned_df, cleaned_df.THB < 3, \"THB minore di 3\")])\n",
    "dirty_df = pd.concat([dirty_df, collect_dirty_rows(cleaned_df, cleaned_df.THB.isnull(), \"THB nullo\")])\n",
    "cleaned_df = cleaned_df.loc[cleaned_df.THB >= 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with uniform values\n",
    "columns_to_drop_uniform = [\"RQ\", \"P50\", \"P50_ST\"]\n",
    "cleaned_df = cleaned_df.drop(columns=columns_to_drop_uniform)\n",
    "\n",
    "# Drop columns with too many null values\n",
    "columns_to_drop_nulls = [\"PEEP\", \"PS\", \"AADPO2\"]\n",
    "cleaned_df = cleaned_df.drop(columns=columns_to_drop_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with any null values\n",
    "cleaned_df = cleaned_df.dropna()\n",
    "\n",
    "# Collect rows with null values and record the reason\n",
    "tmp_dirty = cleaned_df[cleaned_df.isna().any(axis=1)].copy()\n",
    "tmp_dirty[\"Motivo\"] = \"Qualche colonna nulla\"\n",
    "\n",
    "# Append the dirty rows to the dirty_df\n",
    "dirty_df = pd.concat([dirty_df, tmp_dirty], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned_df.shape)\n",
    "print(dirty_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows containing the string '.....' in any column\n",
    "tmp_dirty = cleaned_df[\n",
    "    cleaned_df.apply(lambda row: row.astype(str).str.contains(r'\\.\\.\\.\\.\\.', regex=True)).any(axis=1)\n",
    "].copy()\n",
    "\n",
    "# Add a 'Motivo' column specifying the reason for exclusion\n",
    "tmp_dirty[\"Motivo\"] = \"Qualche colonna contiene .....\"\n",
    "\n",
    "# Append the identified rows to the dirty_df\n",
    "dirty_df = pd.concat([dirty_df, tmp_dirty], ignore_index=True)\n",
    "\n",
    "# Remove rows containing '.....' from the original DataFrame\n",
    "cleaned_df = cleaned_df[\n",
    "    ~cleaned_df.apply(lambda row: row.astype(str).str.contains(r'\\.\\.\\.\\.\\.', regex=True)).any(axis=1)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned_df.shape)\n",
    "print(dirty_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'COD_PATOLOGIA' column to integer\n",
    "cleaned_df['COD_PATOLOGIA'] = cleaned_df['COD_PATOLOGIA'].astype(int)\n",
    "\n",
    "# Apply classification function to 'COD_PATOLOGIA' and create a new column 'class_symptom'\n",
    "cleaned_df['class_symptom'] = cleaned_df['COD_PATOLOGIA'].apply(classify_cod_patology)\n",
    "\n",
    "# Drop the 'COD_PATOLOGIA' column after classification\n",
    "cleaned_df = cleaned_df.drop(columns=['COD_PATOLOGIA'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned_df.shape)\n",
    "print(dirty_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame to a CSV file\n",
    "output_path = f\"../dataset/{DATASET_NAME}.csv\"\n",
    "cleaned_df.to_csv(output_path, index=False)  # Save without the index for a cleaner output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clinicalprobenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
